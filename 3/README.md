# README: Procesamiento de Datos con PySpark

Este documento presenta una gu칤a detallada sobre c칩mo procesar y analizar datos utilizando PySpark. A continuaci칩n se presenta un resumen del contenido del notebook, que incluye operaciones de lectura de datos, agregaciones, limpieza de datos y transformaciones comunes.

## Contenidos del Notebook

### 1. Lectura de Archivos CSV y Combinaci칩n de Datos
   - **Lectura de m칰ltiples archivos CSV**: C칩mo cargar varios archivos de ventas correspondientes a distintos meses del a침o 2019.
   - **Uni칩n de DataFrames**: Combinar todos los archivos le칤dos en un 칰nico DataFrame para un an치lisis conjunto.

### 2. Estad칤sticas Descriptivas sobre Columnas Espec칤ficas
   - **Funci칩n `describe_columns()`**: C칩mo calcular estad칤sticas descriptivas (m칤nimo, m치ximo, media, etc.) de columnas espec칤ficas del DataFrame.

### 3. Agrupaci칩n y Suma de Datos
   - **Funci칩n `group_and_sum()`**: Ejemplo de c칩mo agrupar los datos por una columna y sumar otra columna.

### 4. Filtrado de Datos
   - **Funci칩n `filter_data()`**: Filtrar datos basados en una condici칩n espec칤fica utilizando expresiones SQL.

### 5. A침adir Nuevas Columnas Calculadas
   - **Funci칩n `add_calculated_column()`**: C칩mo agregar nuevas columnas calculadas basadas en operaciones matem치ticas entre columnas existentes.

## Ejemplos Sencillos

### Lectura de M칰ltiples Archivos CSV y Combinaci칩n
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Lista de nombres de archivos
file_paths = [
    "dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/Sales_June_2019.csv",
    "dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/Sales_July_2019.csv",
    "dbfs:/FileStore/shared_uploads/carlosalonsomingo@gmail.com/Sales_March_2019.csv",
    # Otros archivos...
]

# Leer y unir todos los archivos
combined_df = None
for path in file_paths:
    df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load(path)
    combined_df = df if combined_df is None else combined_df.union(df)

combined_df.show()
```
Este ejemplo muestra c칩mo cargar y combinar varios archivos CSV en un 칰nico DataFrame para su an치lisis.

### Calcular Estad칤sticas Descriptivas sobre Columnas Espec칤ficas
```python
from pyspark.sql.functions import col

def describe_columns(df, columns):
    return df.select(columns).describe()

# Ejemplo de uso
stats = describe_columns(combined_df, ["Ventas", "Unidades"])
stats.show()
```
Este ejemplo utiliza la funci칩n `describe_columns()` para calcular estad칤sticas sobre las columnas seleccionadas del DataFrame.

### Agrupar y Sumar Datos
```python
from pyspark.sql import functions as F

def group_and_sum(df, group_column, sum_column):
    return df.groupBy(group_column).agg(F.sum(sum_column).alias(f"sum_{sum_column}"))

# Ejemplo de uso
grouped_data = group_and_sum(combined_df, "Producto", "Ventas")
grouped_data.show()
```
Agrupa los datos por la columna `Producto` y calcula la suma de las ventas correspondientes.

### Filtrar Datos Basados en una Condici칩n
```python
def filter_data(df, condition):
    return df.filter(condition)

# Ejemplo de uso
filtered_df = filter_data(combined_df, "Ventas > 1000")
filtered_df.show()
```
Filtra las filas del DataFrame donde la columna `Ventas` tiene un valor mayor a 1000.

### A침adir una Columna Calculada
```python
from pyspark.sql import functions as F

def add_calculated_column(df, new_column_name, calculation):
    return df.withColumn(new_column_name, calculation)

# Ejemplo de uso
combined_df = add_calculated_column(combined_df, "Revenue", F.col("Unidades") * F.col("Precio"))
combined_df.show()
```
Este ejemplo a침ade una nueva columna `Revenue` calculada multiplicando las columnas `Unidades` y `Precio`.

---

Este README proporciona una gu칤a completa sobre c칩mo realizar operaciones comunes en PySpark, como la lectura y combinaci칩n de archivos, el filtrado y la agregaci칩n de datos, as칤 como la creaci칩n de nuevas columnas. Si necesitas m치s ejemplos o aclaraciones, 춰no dudes en ped칤rmelo! 游땕游

